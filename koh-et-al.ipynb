{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c47f01ae",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/indicium15/ml-workshop/blob/main/koh-et-al.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e421cc29",
   "metadata": {},
   "source": [
    "# Install Dependencies\n",
    "| Package      | Description                                   | Documentation Link |\n",
    "|--------------|-----------------------------------------------|--------------------|\n",
    "| openpyxl     | Read Excel Files                              | [openpyxl docs](https://openpyxl.readthedocs.io/en/stable/) |\n",
    "| pandas       | Data analysis and manipulation library        | [pandas docs](https://pandas.pydata.org/docs/) |\n",
    "| scikit-learn | Machine learning library for Python, including clustering algorithms | [scikit-learn docs](https://scikit-learn.org/stable/) |\n",
    "| matplotlib   | 2D plotting library                           | [matplotlib docs](https://matplotlib.org/stable/) |\n",
    "| gdown        | Package to download files from Google Drive   | [gdown github](https://github.com/wkentaro/gdown) |\n",
    "| numpy        | Fundamental package for scientific computing with Python (arrays, math functions, linear algebra) | [NumPy docs](https://numpy.org/doc/stable/) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0d60cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pandas openpyxl scikit-learn matplotlib seaborn gdown numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea56dd7",
   "metadata": {},
   "source": [
    "# Data Loading\n",
    "In the Excel File, the raw data is stored in the first sheet. Additionally, we must not read the information stored in:\n",
    "1. Column A, as it contains the clustering results, and\n",
    "2. Rows 57-60, as they are not relevant to the analysis.\n",
    "\n",
    "Using Pandas `read_excel` function, which requires the python package `openpyxl`, we can easily load the data into memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4154d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_vps_cluster_data(filepath=\"./data/koh-et-al-2024-data.xlsx\"):\n",
    "    \"\"\"\n",
    "    Loads and preprocesses the 'data_clustered_stats' sheet from the specified Excel file.\n",
    "\n",
    "    - Reads only the 'data_clustered_stats' sheet\n",
    "    - Skips the first column (column A)\n",
    "    - Reads up to row 55\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: The cleaned dataframe\n",
    "    \"\"\"\n",
    "    df = pd.read_excel(\n",
    "        filepath,\n",
    "        sheet_name=\"data_clustered_stats\",\n",
    "        nrows=55\n",
    "    )\n",
    "    # Filter out first column (index 0)\n",
    "    df = df.iloc[:, 1:]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2c7e36",
   "metadata": {},
   "source": [
    "After calling the function to load the data, we can check the `shape` of the data, against our Excel sheet.\n",
    "\n",
    "`shape` is represented as `(number of rows, number of columns)`.\n",
    "\n",
    "We can also use the `.head()` function to inspect the first five rows of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9773f30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_vps_cluster_data()\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c9be75",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "An important part of data pre-processing is filtering out NaN (empty) cells. If not handled, they can cause errors in downstream machine learning tasks.\n",
    "\n",
    "Thankfully, it is very easy to check for these values using the `isna()` method in Pandas, and find their locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a07beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NaN values and their locations and columns\n",
    "nan_columns = df.isna().any()\n",
    "print(\"Columns with NaN values:\")\n",
    "print(nan_columns[nan_columns].index.tolist())\n",
    "nan_locations = df.isna().any(axis=1)\n",
    "print(\"Rows with NaN values:\")\n",
    "print(df[nan_locations])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01737db8",
   "metadata": {},
   "source": [
    "In this case, we can safely assume that an empty cell reflects a zero value. Again, pandas makes this very easy with the `fillna()` function, and we can choose what value to fill the empty cells with as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfd4a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaN values with 0\n",
    "df = df.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51170e48",
   "metadata": {},
   "source": [
    "# Data Visualization\n",
    "Pandas and Matplotlib are powerful tools for performing exploratory data analysis and visualizing results.\n",
    "\n",
    "For example, here, with just a few lines of code, we can create histograms for each column and plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8d7f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Create histograms for each column\n",
    "df.hist(figsize=(15,10), bins=20)\n",
    "# Visualize distributions for each column\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b13f84",
   "metadata": {},
   "source": [
    "# K-Means Clustering with Calinski–Harabasz Optimization\n",
    "\n",
    "In this section, we:\n",
    "\n",
    "1. Determine the optimal number of clusters (`k`) \n",
    "   - We run K-Means for a range of possible `k` values (default: 2–9).\n",
    "   - For each `k`, we compute the **Calinski–Harabasz (CH) score**, which measures how well the data is separated into clusters (higher = better separation).\n",
    "   - We plot the CH scores so you can visually inspect which `k` performs best.\n",
    "   - The function prints and returns the `k` with the highest CH score.\n",
    "\n",
    "2. Run K-Means clustering  \n",
    "   - If `k` is not provided, we automatically select the best `k` using the CH score.\n",
    "   - We fit K-Means to the dataset and assign each sample to a cluster.\n",
    "   - The cluster labels are added as a new column (`Cluster`) in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cb9131",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import calinski_harabasz_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def find_optimal_k(data, k_range=range(2, 10)):\n",
    "    \"\"\"\n",
    "    Use Calinski-Harabasz score to find optimal number of clusters.\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    for k in k_range:\n",
    "        model = KMeans(n_clusters=k, random_state=42)\n",
    "        labels = model.fit_predict(data)\n",
    "        score = calinski_harabasz_score(data, labels)\n",
    "        scores.append(score)\n",
    "    \n",
    "    # Plot CH scores\n",
    "    plt.plot(k_range, scores, marker='o')\n",
    "    plt.xlabel(\"Number of clusters (k)\")\n",
    "    plt.ylabel(\"Calinski-Harabasz Score\")\n",
    "    plt.title(\"Optimal k via Calinski-Harabasz\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # Return best k\n",
    "    best_k = k_range[scores.index(max(scores))]\n",
    "    print(f\"Optimal k based on CH score: {best_k}\")\n",
    "    return best_k\n",
    "\n",
    "def cluster_vps_data(df, k=None):\n",
    "    \"\"\"\n",
    "    Full pipeline: choose optimal k (if not given), cluster with KMeans.\n",
    "    \n",
    "    Returns:\n",
    "        clustered_df: dataframe with cluster labels\n",
    "        model: fitted KMeans model\n",
    "    \"\"\"\n",
    "    if k is None:\n",
    "        k = find_optimal_k(df)\n",
    "    \n",
    "    model = KMeans(n_clusters=k, random_state=42)\n",
    "    labels = model.fit_predict(df)\n",
    "    \n",
    "    clustered_df = df.copy()\n",
    "    clustered_df['Cluster'] = labels\n",
    "    \n",
    "    return clustered_df, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd65792a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_df, kmeans_model = cluster_vps_data(df)\n",
    "# View cluster sizes\n",
    "print(clustered_df['Cluster'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722a039c",
   "metadata": {},
   "source": [
    "# Visualize Clustering Results\n",
    "After creating a new `Cluster` column in our DataFrame, we can summarize and visualize the characteristics of each cluster to understand how they differ.\n",
    "\n",
    "### Group Data with `pandas.groupby`\n",
    "- `groupby('Cluster')` groups the dataset by the cluster labels we assigned earlier.\n",
    "- Using `.mean(numeric_only=True)` calculates the average value of each numeric feature for every cluster.\n",
    "- Simlarly, `.std(numeric_only=True)` calculates the standard deviation of each feature for every cluster.\n",
    "\n",
    "- These summaries are helpful for profiling clusters:  \n",
    "  - High mean: feature is typical in that cluster.  \n",
    "  - Low mean: feature is uncommon in that cluster.  \n",
    "  - High std: feature usage varies widely between members of the cluster.  \n",
    "  - Low std: feature usage is consistent in that cluster.\n",
    "\n",
    "The code block below plots a grid of bar charts, where each row corresponds to one feature, and each column corresponds to one cluter. Bars represent cluster means, while the error bars show the standard deviation. This makes it easy to compare profiles across clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a1e2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compute means & stds per cluster\n",
    "cluster_means = clustered_df.groupby('Cluster').mean(numeric_only=True)\n",
    "cluster_stds  = clustered_df.groupby('Cluster').std(numeric_only=True)\n",
    "\n",
    "# Order axes\n",
    "clusters = cluster_means.index.tolist()\n",
    "variables = cluster_means.columns.tolist()\n",
    "\n",
    "n_rows = len(variables)   # Variables as rows\n",
    "n_cols = len(clusters)    # Clusters as columns\n",
    "\n",
    "# Figure size big enough to fit all variables\n",
    "fig_w = max(12, n_cols * 2.0)\n",
    "fig_h = max(6, n_rows * 2.0)\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(fig_w, fig_h), squeeze=False)\n",
    "\n",
    "for i, var in enumerate(variables):\n",
    "    col_min = (cluster_means[var] - cluster_stds[var]).min()\n",
    "    col_max = (cluster_means[var] + cluster_stds[var]).max()\n",
    "    margin = 0.05 * (col_max - col_min if col_max > col_min else 1.0)\n",
    "    col_ylim = (col_min - margin, col_max + margin)\n",
    "\n",
    "    for j, cl in enumerate(clusters):\n",
    "        ax = axes[i, j]\n",
    "        mean_val = cluster_means.loc[cl, var]\n",
    "        std_val  = cluster_stds.loc[cl, var]\n",
    "\n",
    "        ax.bar(0, mean_val, yerr=std_val, capsize=4)\n",
    "        ax.set_ylim(col_ylim)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_xlim(-0.6, 0.6)\n",
    "\n",
    "        if i == 0:\n",
    "            ax.set_title(f\"Cluster {cl}\", fontsize=11, pad=10)\n",
    "        if j == 0:\n",
    "            ax.set_ylabel(var, fontsize=11)\n",
    "\n",
    "        ax.yaxis.grid(True, linestyle=\":\", alpha=0.5)\n",
    "\n",
    "fig.suptitle(\"Per-Variable Mean ± STD\", fontsize=14, y=1.005)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e72d5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Try it yourself: K-Means with manual or automatic K selection {display-mode: \"form\"}\n",
    "#@markdown Choose how to select **K** (manual or via Elbow / Silhouette / Calinski–Harabasz), optionally normalize features to 0–1, and then press ▶️.\n",
    "select_k_mode = \"Manual (use slider)\"  #@param [\"Manual (use slider)\", \"Auto: Elbow Method\", \"Auto: Silhouette Score\", \"Auto: Calinski–Harabasz Index\"]\n",
    "k_manual = 3  #@param {type:\"slider\", min:2, max:12, step:1}\n",
    "k_min = 2  #@param {type:\"slider\", min:2, max:2, step:1}\n",
    "k_max = 10  #@param {type:\"slider\", min:3, max:20, step:1}\n",
    "min_max_normalization = True  #@param {type:\"boolean\"}\n",
    "\n",
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
    "\n",
    "random_state = 42\n",
    "\n",
    "if 'df' not in globals():\n",
    "    raise RuntimeError(\"This cell expects a pandas DataFrame named `df` to exist.\")\n",
    "\n",
    "# Keep only numeric columns for clustering\n",
    "X = df.select_dtypes(include=[np.number]).copy()\n",
    "if X.shape[1] == 0:\n",
    "    raise ValueError(\"No numeric columns found in `df` to run k-means.\")\n",
    "\n",
    "# Optional normalization\n",
    "if min_max_normalization:\n",
    "    scaler = MinMaxScaler()\n",
    "    X = pd.DataFrame(\n",
    "        scaler.fit_transform(X),\n",
    "        columns=X.columns,\n",
    "        index=X.index\n",
    "    )\n",
    "\n",
    "n_samples = X.shape[0]\n",
    "if k_max >= n_samples:\n",
    "    k_max = max(2, n_samples - 1) \n",
    "if k_min >= k_max:\n",
    "    k_min = max(2, min(k_max - 1, 2))\n",
    "\n",
    "def run_kmeans(X, k, random_state=42):\n",
    "    km = KMeans(n_clusters=k, n_init='auto', random_state=random_state)\n",
    "    labels = km.fit_predict(X)\n",
    "    return km, labels\n",
    "\n",
    "# Let the user select K manually or automatically\n",
    "chosen_k = None\n",
    "score_curve = None\n",
    "score_label = None\n",
    "k_values = list(range(k_min, k_max + 1))\n",
    "\n",
    "if select_k_mode == \"Manual (use slider)\":\n",
    "    chosen_k = int(k_manual)\n",
    "else:\n",
    "    scores = []\n",
    "    for k in k_values:\n",
    "        km, lbl = run_kmeans(X, k, random_state=random_state)\n",
    "        if select_k_mode == \"Auto: Elbow Method\":\n",
    "            # WCSS (inertia) — lower is better\n",
    "            scores.append(km.inertia_)\n",
    "            score_label = \"Within-Cluster SSE (Inertia) ↓\"\n",
    "        elif select_k_mode == \"Auto: Silhouette Score\":\n",
    "            # Average silhouette\n",
    "            s = silhouette_score(X, lbl)\n",
    "            scores.append(s)\n",
    "            score_label = \"Silhouette Score ↑\"\n",
    "        else:  \n",
    "            # Calculate Calinski–Harabasz Index\n",
    "            s = calinski_harabasz_score(X, lbl)\n",
    "            scores.append(s)\n",
    "            score_label = \"Calinski–Harabasz Index ↑\"\n",
    "    score_curve = np.array(scores)\n",
    "\n",
    "    # Pick K:\n",
    "    if select_k_mode == \"Auto: Elbow Method\":\n",
    "        # Simple elbow heuristic via first-derivative drop (argmin of second derivative)\n",
    "        # Compute discrete second derivative and pick its minimum (largest curvature)\n",
    "        y = score_curve\n",
    "        if len(k_values) >= 3:\n",
    "            second_deriv = np.diff(y, n=2)\n",
    "            # Find minimum index of second derivative\n",
    "            idx = np.argmin(second_deriv) + 1  \n",
    "            chosen_k = k_values[idx]\n",
    "        else:\n",
    "            chosen_k = k_values[np.argmin(np.gradient(y))] \n",
    "    else:\n",
    "        # For Silhouette / CH pick the argmax\n",
    "        chosen_k = k_values[int(np.argmax(score_curve))]\n",
    "\n",
    "print(f\"Selected K mode: {select_k_mode}\")\n",
    "print(f\"Chosen k: {chosen_k}\")\n",
    "\n",
    "# Final Clustering\n",
    "final_kmeans, final_labels = run_kmeans(X, chosen_k, random_state=random_state)\n",
    "clustered_df = df.copy()\n",
    "clustered_df['Cluster'] = final_labels\n",
    "\n",
    "# Show cluster sizes\n",
    "print(\"\\nCluster sizes:\")\n",
    "print(clustered_df['Cluster'].value_counts().sort_index(), \"\\n\")\n",
    "\n",
    "# Score Visualization \n",
    "if select_k_mode != \"Manual (use slider)\" and score_curve is not None:\n",
    "    plt.figure(figsize=(7, 4.5))\n",
    "    plt.plot(k_values, score_curve, marker='o')\n",
    "    if select_k_mode == \"Auto: Elbow Method\":\n",
    "        plt.title(\"Elbow Method: Inertia vs K\")\n",
    "        plt.ylabel(\"Inertia (WCSS) ↓\")\n",
    "    elif select_k_mode == \"Auto: Silhouette Score\":\n",
    "        plt.title(\"Silhouette Score vs K\")\n",
    "        plt.ylabel(\"Average Silhouette ↑\")\n",
    "    else:\n",
    "        plt.title(\"Calinski–Harabasz Index vs K\")\n",
    "        plt.ylabel(\"CH Index ↑\")\n",
    "    plt.xlabel(\"K (number of clusters)\")\n",
    "    # Mark chosen K\n",
    "    try:\n",
    "        idx = k_values.index(chosen_k)\n",
    "        plt.scatter([chosen_k], [score_curve[idx]], s=120, zorder=3)\n",
    "    except ValueError:\n",
    "        pass\n",
    "    plt.grid(True, linestyle=':', alpha=0.6)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize Cluster Means & STDs\n",
    "cluster_means = clustered_df.groupby('Cluster').mean(numeric_only=True)\n",
    "cluster_stds  = clustered_df.groupby('Cluster').std(numeric_only=True)\n",
    "# Order axes\n",
    "clusters = cluster_means.index.tolist()\n",
    "variables = cluster_means.columns.tolist()\n",
    "\n",
    "n_rows = len(variables)\n",
    "n_cols = len(clusters)\n",
    "\n",
    "fig_w = max(12, n_cols * 2.0)\n",
    "fig_h = max(6,  n_rows * 2.0)\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(fig_w, fig_h), squeeze=False)\n",
    "\n",
    "for i, var in enumerate(variables):\n",
    "    # Shared y-limits across clusters for this variable\n",
    "    col_min = (cluster_means[var] - cluster_stds[var]).min()\n",
    "    col_max = (cluster_means[var] + cluster_stds[var]).max()\n",
    "    margin = 0.05 * (col_max - col_min if col_max > col_min else 1.0)\n",
    "    col_ylim = (col_min - margin, col_max + margin)\n",
    "\n",
    "    for j, cl in enumerate(clusters):\n",
    "        ax = axes[i, j]\n",
    "        mean_val = float(cluster_means.loc[cl, var])\n",
    "        std_val  = float(cluster_stds.loc[cl, var]) if not np.isnan(cluster_stds.loc[cl, var]) else 0.0\n",
    "\n",
    "        ax.bar(0, mean_val, yerr=std_val, capsize=4)\n",
    "        ax.set_ylim(col_ylim)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_xlim(-0.6, 0.6)\n",
    "\n",
    "        if i == 0:\n",
    "            ax.set_title(f\"Cluster {cl}\", fontsize=11, pad=10)\n",
    "        if j == 0:\n",
    "            ax.set_ylabel(var, fontsize=11)\n",
    "\n",
    "        ax.yaxis.grid(True, linestyle=\":\", alpha=0.5)\n",
    "\n",
    "fig.suptitle(f\"Per-Variable Mean ± STD (K = {chosen_k})\", fontsize=14, y=1.005)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
